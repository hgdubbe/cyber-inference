"""
Model management for Cyber-Inference.

Handles:
- HuggingFace model discovery and download
- Local model registration and tracking
- Model metadata extraction
- Download progress tracking
"""

import asyncio
import os
import re
from datetime import datetime
from pathlib import Path
from typing import AsyncGenerator, Optional

import httpx
from huggingface_hub import HfApi, hf_hub_download, list_repo_files
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from cyber_inference.core.config import get_settings
from cyber_inference.core.database import get_db_session
from cyber_inference.core.logging import get_logger
from cyber_inference.models.db_models import Model

logger = get_logger(__name__)


class ModelManager:
    """
    Manages model discovery, download, and registration.

    Integrates with HuggingFace Hub for model downloads
    and maintains local database of available models.
    """

    def __init__(self, models_dir: Optional[Path] = None):
        """
        Initialize the model manager.

        Args:
            models_dir: Directory for storing models
        """
        settings = get_settings()
        self.models_dir = models_dir or settings.models_dir
        self.models_dir.mkdir(parents=True, exist_ok=True)

        self._hf_token = settings.hf_token
        self._hf_api = HfApi(token=self._hf_token)

        logger.info("[info]ModelManager initialized[/info]")
        logger.debug(f"  Models directory: {self.models_dir}")
        logger.debug(f"  HuggingFace token: {'configured' if self._hf_token else 'not set'}")

    @staticmethod
    def _read_gguf_context_length(file_path: Path) -> Optional[int]:
        def read_exact(handle, size: int) -> bytes:
            data = handle.read(size)
            if len(data) != size:
                raise ValueError("Unexpected end of file")
            return data

        def read_u8(handle) -> int:
            return int.from_bytes(read_exact(handle, 1), "little", signed=False)

        def read_i8(handle) -> int:
            return int.from_bytes(read_exact(handle, 1), "little", signed=True)

        def read_u16(handle) -> int:
            return int.from_bytes(read_exact(handle, 2), "little", signed=False)

        def read_i16(handle) -> int:
            return int.from_bytes(read_exact(handle, 2), "little", signed=True)

        def read_u32(handle) -> int:
            return int.from_bytes(read_exact(handle, 4), "little", signed=False)

        def read_u64(handle) -> int:
            return int.from_bytes(read_exact(handle, 8), "little", signed=False)

        def read_i32(handle) -> int:
            return int.from_bytes(read_exact(handle, 4), "little", signed=True)

        def read_i64(handle) -> int:
            return int.from_bytes(read_exact(handle, 8), "little", signed=True)

        def read_string(handle) -> str:
            length = read_u64(handle)
            if length == 0:
                return ""
            return read_exact(handle, length).decode("utf-8", errors="ignore")

        def skip_string(handle) -> None:
            length = read_u64(handle)
            if length:
                read_exact(handle, length)

        type_sizes = {
            0: 1,  # UINT8
            1: 1,  # INT8
            2: 2,  # UINT16
            3: 2,  # INT16
            4: 4,  # UINT32
            5: 4,  # INT32
            6: 4,  # FLOAT32
            7: 1,  # BOOL
            10: 8,  # UINT64
            11: 8,  # INT64
            12: 8,  # FLOAT64
        }
        GGUF_STRING = 8
        GGUF_ARRAY = 9

        context_keys = {
            "llama.context_length",
            "context_length",
            "n_ctx",
            "llama.n_ctx",
            "n_ctx_train",
            "llama.n_ctx_train",
        }

        def is_context_key(key: str) -> bool:
            if key in context_keys:
                return True
            if key.endswith((".context_length", ".n_ctx", ".n_ctx_train")):
                return True
            return False

        def skip_array(handle, elem_type: int, length: int) -> None:
            if elem_type == GGUF_STRING:
                for _ in range(length):
                    skip_string(handle)
                return
            elem_size = type_sizes.get(elem_type)
            if elem_size is None:
                raise ValueError("Unknown GGUF array element type")
            read_exact(handle, elem_size * length)

        def skip_value(handle, value_type: int) -> None:
            if value_type == GGUF_STRING:
                skip_string(handle)
                return
            if value_type == GGUF_ARRAY:
                elem_type = read_u32(handle)
                length = read_u64(handle)
                skip_array(handle, elem_type, length)
                return
            size = type_sizes.get(value_type)
            if size is None:
                raise ValueError("Unknown GGUF value type")
            read_exact(handle, size)

        def read_numeric_value(handle, value_type: int) -> Optional[int]:
            if value_type == 0:
                return read_u8(handle)
            if value_type == 1:
                return read_i8(handle)
            if value_type == 2:
                return read_u16(handle)
            if value_type == 3:
                return read_i16(handle)
            if value_type == 4:
                return read_u32(handle)
            if value_type == 5:
                return read_i32(handle)
            if value_type == 7:
                return read_u8(handle)
            if value_type == 10:
                return read_u64(handle)
            if value_type == 11:
                return read_i64(handle)
            if value_type == GGUF_STRING:
                value = read_string(handle)
                if value.isdigit():
                    return int(value)
                return None
            if value_type == GGUF_ARRAY:
                elem_type = read_u32(handle)
                length = read_u64(handle)
                if length == 1:
                    return read_numeric_value(handle, elem_type)
                skip_array(handle, elem_type, length)
                return None
            skip_value(handle, value_type)
            return None

        def select_context_length(
            architecture: Optional[str],
            candidates: dict[str, int],
        ) -> Optional[int]:
            if not candidates:
                return None

            if architecture:
                arch_variants = {
                    architecture,
                    architecture.replace("-", "_"),
                    architecture.replace("_", "-"),
                }
                for arch in arch_variants:
                    for suffix in ("context_length", "n_ctx", "n_ctx_train"):
                        key = f"{arch}.{suffix}"
                        if key in candidates:
                            return candidates[key]

            for key in (
                "llama.context_length",
                "llama.n_ctx",
                "llama.n_ctx_train",
                "context_length",
                "n_ctx",
                "n_ctx_train",
            ):
                if key in candidates:
                    return candidates[key]

            for key, value in candidates.items():
                if key.startswith(("clip.", "vision.", "mmproj.")):
                    continue
                return value

            return None

        try:
            with file_path.open("rb") as handle:
                magic = read_exact(handle, 4)
                if magic != b"GGUF":
                    return None

                _version = read_u32(handle)
                _tensor_count = read_u64(handle)
                kv_count = read_u64(handle)

                architecture = None
                candidates: dict[str, int] = {}

                for _ in range(kv_count):
                    key = read_string(handle)
                    key_lower = key.lower()
                    value_type = read_u32(handle)

                    if key_lower == "general.architecture":
                        if value_type == GGUF_STRING:
                            architecture = read_string(handle).lower()
                        else:
                            skip_value(handle, value_type)
                        continue

                    if is_context_key(key_lower):
                        value = read_numeric_value(handle, value_type)
                        if value is not None:
                            candidates[key_lower] = int(value)
                        continue

                    skip_value(handle, value_type)

                return select_context_length(architecture, candidates)
        except Exception as e:
            logger.debug(f"Failed to read GGUF metadata from {file_path.name}: {e}")
            return None

        return None

    @staticmethod
    def _is_mmproj_file(filename: str) -> bool:
        name = filename.lower()
        return name.endswith(".gguf") and "mmproj" in name

    @staticmethod
    def _split_repo_and_filename(
        repo_id: str,
        filename: Optional[str],
    ) -> tuple[str, Optional[str]]:
        if filename:
            return repo_id, filename

        cleaned = repo_id.strip()
        if cleaned.startswith("https://huggingface.co/"):
            cleaned = cleaned[len("https://huggingface.co/"):]
        cleaned = cleaned.lstrip("/").split("?", 1)[0]

        if not cleaned.endswith(".gguf"):
            return repo_id, filename

        parts = cleaned.split("/")
        if len(parts) < 3:
            return repo_id, filename

        for marker in ("blob", "resolve", "tree"):
            if marker in parts:
                idx = parts.index(marker)
                if idx + 1 < len(parts):
                    parts = parts[:idx] + parts[idx + 2:]
                else:
                    parts = parts[:idx]
                break

        if len(parts) < 3 or not parts[-1].endswith(".gguf"):
            return repo_id, filename

        new_repo_id = "/".join(parts[:2])
        new_filename = "/".join(parts[2:])
        return new_repo_id, new_filename

    @staticmethod
    def _select_mmproj_file(files: list[str], model_filename: str) -> Optional[str]:
        mmproj_files = sorted([f for f in files if ModelManager._is_mmproj_file(f)])
        if not mmproj_files:
            return None

        def basename(path: str) -> str:
            return Path(path).name

        def pick_by_basename(names: list[str]) -> Optional[str]:
            preferred = (
                "mmproj-f16.gguf",
                "mmproj-f32.gguf",
                "mmproj-q8_0.gguf",
                "mmproj-q4_0.gguf",
            )
            lowered = {basename(name).lower(): name for name in names}
            for candidate in preferred:
                if candidate in lowered:
                    return lowered[candidate]
            return None

        model_stem = Path(model_filename).stem
        exact = f"mmproj-{model_stem}.gguf".lower()
        for candidate in mmproj_files:
            if basename(candidate).lower() == exact:
                return candidate

        base_name = re.sub(r"(?i)-q\d+.*$", "", model_stem)
        prefix = f"mmproj-{base_name}".lower()
        prefixed = [f for f in mmproj_files if basename(f).lower().startswith(prefix)]
        if len(prefixed) == 1:
            return prefixed[0]
        if prefixed:
            return sorted(prefixed, key=lambda name: len(basename(name)))[0]

        if len(mmproj_files) == 1:
            return mmproj_files[0]

        generic = pick_by_basename(mmproj_files)
        if generic:
            return generic

        contains = [f for f in mmproj_files if base_name.lower() in basename(f).lower()]
        if len(contains) == 1:
            return contains[0]
        if contains:
            return sorted(contains, key=lambda name: len(basename(name)))[0]

        logger.warning(
            "[warning]Multiple mmproj files found but none matched model %s[/warning]",
            model_stem,
        )
        return None

    async def _download_mmproj(
        self,
        repo_id: str,
        model_path: Path,
        repo_files: Optional[list[str]] = None,
        force: bool = False,
    ) -> Optional[Path]:
        try:
            files = repo_files or list_repo_files(repo_id, token=self._hf_token)
        except Exception as e:
            logger.warning(f"[warning]Could not list repo files for mmproj: {e}[/warning]")
            return None

        mmproj_filename = self._select_mmproj_file(files, model_path.name)
        if not mmproj_filename:
            return None

        local_path = model_path.parent / f"mmproj-{model_path.stem}.gguf"
        if local_path.exists() and not force:
            logger.info(f"  mmproj already present: {local_path}")
            return local_path

        logger.info(f"  Downloading mmproj: {mmproj_filename}")
        try:
            downloaded_path = await asyncio.to_thread(
                hf_hub_download,
                repo_id=repo_id,
                filename=mmproj_filename,
                local_dir=self.models_dir,
                local_dir_use_symlinks=False,
                token=self._hf_token,
            )
            downloaded_path = Path(downloaded_path)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            if downloaded_path != local_path:
                if local_path.exists():
                    local_path.unlink()
                downloaded_path.rename(local_path)
                logger.info(f"  Aligned mmproj filename to: {local_path.name}")
            logger.info(f"[success]mmproj download complete: {local_path}[/success]")
            return local_path
        except Exception as e:
            logger.warning(f"[warning]mmproj download failed: {e}[/warning]")
            return None

    async def search_models(
        self,
        query: str,
        limit: int = 20,
    ) -> list[dict]:
        """
        Search HuggingFace for GGUF models.

        Args:
            query: Search query
            limit: Maximum results to return

        Returns:
            List of model information dicts
        """
        logger.info(f"[info]Searching HuggingFace for: {query}[/info]")

        try:
            # Search for GGUF models
            models = list(self._hf_api.list_models(
                search=query,
                filter="gguf",
                limit=limit,
                sort="downloads",
                direction=-1,
            ))

            results = []
            for model in models:
                results.append({
                    "id": model.id,
                    "author": model.author,
                    "downloads": model.downloads,
                    "likes": model.likes,
                    "tags": model.tags,
                    "last_modified": model.last_modified,
                })

            logger.info(f"[success]Found {len(results)} models[/success]")
            return results

        except Exception as e:
            logger.error(f"[error]HuggingFace search failed: {e}[/error]")
            raise

    async def list_repo_files(self, repo_id: str, files: Optional[list[str]] = None) -> list[dict]:
        """
        List GGUF files in a HuggingFace repository.

        Args:
            repo_id: HuggingFace repository ID

        Returns:
            List of file information dicts
        """
        logger.info(f"[info]Listing files in repo: {repo_id}[/info]")

        try:
            files = files or list_repo_files(repo_id, token=self._hf_token)

            gguf_files = []
            for filename in files:
                if filename.endswith(".gguf") and not self._is_mmproj_file(filename):
                    # Get file info
                    try:
                        info = self._hf_api.get_hf_file_metadata(
                            f"https://huggingface.co/{repo_id}/resolve/main/{filename}"
                        )
                        size = info.size if hasattr(info, 'size') else 0
                    except Exception:
                        size = 0

                    # Extract quantization from filename
                    quant_match = re.search(r'[qQ](\d+)_?([kKmM])?', filename)
                    quantization = quant_match.group(0) if quant_match else None

                    gguf_files.append({
                        "filename": filename,
                        "size_bytes": size,
                        "quantization": quantization,
                    })

            logger.info(f"[success]Found {len(gguf_files)} GGUF files[/success]")
            return gguf_files

        except Exception as e:
            logger.error(f"[error]Failed to list repo files: {e}[/error]")
            raise

    async def download_model(
        self,
        repo_id: str,
        filename: Optional[str] = None,
        force: bool = False,
    ) -> Path:
        """
        Download a model from HuggingFace.

        Args:
            repo_id: HuggingFace repository ID
            filename: Specific file to download (auto-detect if None)
            force: Force redownload even if exists

        Returns:
            Path to downloaded model file
        """
        repo_id = repo_id.strip()
        filename = filename.strip() if filename else None
        parsed = self._split_repo_and_filename(repo_id, filename)
        repo_id, filename = parsed

        logger.info(f"[highlight]Downloading model from: {repo_id}[/highlight]")
        repo_files = list_repo_files(repo_id, token=self._hf_token)

        # If no filename specified, find the best GGUF file
        if filename is None:
            files = await self.list_repo_files(repo_id, files=repo_files)
            if not files:
                raise ValueError(f"No GGUF files found in {repo_id}")

            # Prefer Q4_K_M or similar balanced quantization
            preferred_quants = ["Q4_K_M", "Q4_K_S", "Q5_K_M", "Q5_K_S", "Q4_0"]
            filename = files[0]["filename"]

            for quant in preferred_quants:
                for f in files:
                    if quant.lower() in f["filename"].lower():
                        filename = f["filename"]
                        break
                else:
                    continue
                break

            logger.info(f"  Auto-selected file: {filename}")

        # Notify download starting
        await self._notify_progress(repo_id, filename, 0, "starting")

        # Check if already downloaded
        local_path = self.models_dir / filename
        if local_path.exists() and not force:
            logger.info(f"[success]Model already exists: {local_path}[/success]")

            # Ensure it's registered in DB
            await self._register_model(repo_id, filename, local_path)
            await self._download_mmproj(repo_id, local_path, repo_files=repo_files, force=force)

            # Notify complete
            await self._notify_progress(repo_id, filename, 100, "complete")

            return local_path

        # Download with progress
        logger.info(f"  Downloading to: {local_path}")

        try:
            # Create a progress tracker
            last_progress = [0]  # Use list to allow modification in nested function

            def progress_callback(current: int, total: int) -> None:
                if total > 0:
                    progress = int((current / total) * 100)
                    # Only log every 10%
                    if progress >= last_progress[0] + 10:
                        last_progress[0] = progress
                        logger.info(f"  Download progress: {progress}%")
                        # Schedule async notification
                        try:
                            asyncio.get_event_loop().create_task(
                                self._notify_progress(repo_id, filename, progress, "downloading")
                            )
                        except RuntimeError:
                            pass  # No event loop available

            # Notify downloading
            await self._notify_progress(repo_id, filename, 5, "downloading")

            # Use huggingface_hub for download with progress
            downloaded_path = await asyncio.to_thread(
                hf_hub_download,
                repo_id=repo_id,
                filename=filename,
                local_dir=self.models_dir,
                local_dir_use_symlinks=False,
                token=self._hf_token,
            )

            # Move to expected location if needed
            downloaded_path = Path(downloaded_path)
            if downloaded_path != local_path:
                downloaded_path.rename(local_path)

            logger.info(f"[success]Download complete: {local_path}[/success]")

            # Register in database
            await self._register_model(repo_id, filename, local_path)
            await self._download_mmproj(repo_id, local_path, repo_files=repo_files, force=force)

            # Notify complete
            await self._notify_progress(repo_id, filename, 100, "complete")

            return local_path

        except Exception as e:
            logger.error(f"[error]Download failed: {e}[/error]")
            # Clean up partial download
            if local_path.exists():
                try:
                    local_path.unlink()
                    logger.info(f"  Cleaned up partial file: {local_path}")
                except Exception as cleanup_err:
                    logger.warning(f"  Could not clean up partial file: {cleanup_err}")
            # Notify error
            await self._notify_progress(repo_id, filename, 0, "error", str(e))
            raise

    async def _notify_progress(
        self,
        repo_id: str,
        filename: str,
        progress: float,
        status: str,
        error: Optional[str] = None,
    ) -> None:
        """Send download progress notification via WebSocket."""
        try:
            from cyber_inference.api.websocket import notify_download_progress
            await notify_download_progress(repo_id, filename, progress, status, error)
        except Exception as e:
            logger.debug(f"Could not send progress notification: {e}")

    async def _register_model(
        self,
        repo_id: str,
        filename: str,
        file_path: Path,
    ) -> Model:
        """Register a model in the database."""
        logger.debug(f"Registering model: {filename}")

        # Extract info from filename
        quant_match = re.search(r'[qQ](\d+)_?([kKmM])?', filename)
        quantization = quant_match.group(0) if quant_match else None

        # Get file size
        size_bytes = file_path.stat().st_size if file_path.exists() else 0

        # Determine model name (use filename without extension)
        model_name = filename.replace(".gguf", "")
        context_length = self._read_gguf_context_length(file_path)

        async with get_db_session() as session:
            # Check if already registered
            result = await session.execute(
                select(Model).where(Model.name == model_name)
            )
            existing = result.scalar_one_or_none()

            if existing:
                # Update existing record
                existing.file_path = str(file_path)
                existing.size_bytes = size_bytes
                existing.is_downloaded = True
                existing.download_progress = 100.0
                if context_length:
                    existing.context_length = context_length

                # Auto-detect model type if not set
                if not existing.model_type:
                    name_lower = model_name.lower()
                    embedding_patterns = ["embed", "bge", "e5-", "gte-", "stella", "nomic"]
                    if any(pattern in name_lower for pattern in embedding_patterns):
                        existing.model_type = "embedding"
                        logger.info(f"  Auto-detected model type: embedding")

                await session.commit()
                logger.debug(f"Updated existing model record: {model_name}")
                return existing

            # Auto-detect model type from name
            model_type = None
            name_lower = model_name.lower()
            embedding_patterns = ["embed", "bge", "e5-", "gte-", "stella", "nomic"]
            if any(pattern in name_lower for pattern in embedding_patterns):
                model_type = "embedding"
                logger.info(f"  Auto-detected model type: embedding")

            # Create new record
            model = Model(
                name=model_name,
                filename=filename,
                file_path=str(file_path),
                hf_repo_id=repo_id,
                hf_filename=filename,
                size_bytes=size_bytes,
                quantization=quantization,
                context_length=context_length or 4096,
                model_type=model_type,
                is_downloaded=True,
                download_progress=100.0,
            )
            session.add(model)
            await session.commit()

            logger.info(f"[success]Model registered: {model_name}[/success]")
            return model

    async def list_models(self) -> list[dict]:
        """
        List all models (registered and local files).

        Returns:
            List of model information dicts
        """
        logger.debug("Listing all models")

        models = []

        # Get models from database
        async with get_db_session() as session:
            result = await session.execute(select(Model))
            db_models = result.scalars().all()

            updated = False
            for model in db_models:
                if model.file_path and (model.context_length is None or model.context_length == 4096):
                    file_path = Path(model.file_path)
                    if file_path.exists() and file_path.suffix == ".gguf":
                        context_length = self._read_gguf_context_length(file_path)
                        if context_length and context_length != model.context_length:
                            model.context_length = context_length
                            updated = True

                models.append({
                    "id": model.id,
                    "name": model.name,
                    "filename": model.filename,
                    "path": model.file_path,
                    "hf_repo_id": model.hf_repo_id,
                    "size_bytes": model.size_bytes,
                    "quantization": model.quantization,
                    "context_length": model.context_length,
                    "model_type": model.model_type,
                    "is_downloaded": model.is_downloaded,
                    "is_enabled": model.is_enabled,
                    "last_used_at": model.last_used_at,
                    "registered": True,
                })

            if updated:
                await session.commit()

        # Scan for unregistered local files
        registered_files = {m["filename"] for m in models}

        for file_path in self.models_dir.glob("*.gguf"):
            if self._is_mmproj_file(file_path.name):
                continue
            if file_path.name not in registered_files:
                context_length = self._read_gguf_context_length(file_path)
                models.append({
                    "id": None,
                    "name": file_path.stem,
                    "filename": file_path.name,
                    "path": str(file_path),
                    "hf_repo_id": None,
                    "size_bytes": file_path.stat().st_size,
                    "quantization": None,
                    "context_length": context_length or 4096,
                    "model_type": None,
                    "is_downloaded": True,
                    "is_enabled": True,
                    "last_used_at": None,
                    "registered": False,
                })

        logger.debug(f"Found {len(models)} models")
        return models

    async def get_model(self, name: str) -> Optional[dict]:
        """
        Get model by name.

        Args:
            name: Model name

        Returns:
            Model information dict, or None if not found
        """
        models = await self.list_models()
        for model in models:
            if model["name"] == name:
                return model
        return None

    async def get_model_path(self, name: str) -> Optional[Path]:
        """
        Get the file path for a model.

        Args:
            name: Model name

        Returns:
            Path to model file, or None if not found
        """
        model = await self.get_model(name)
        if model and model["is_downloaded"]:
            return Path(model["path"])
        return None

    async def delete_model(self, name: str) -> bool:
        """
        Delete a model (file and database record).

        Args:
            name: Model name

        Returns:
            True if deleted, False if not found
        """
        logger.info(f"[warning]Deleting model: {name}[/warning]")

        model = await self.get_model(name)
        if not model:
            logger.warning(f"Model not found: {name}")
            return False

        # Delete file (ignore if already gone)
        file_path = Path(model["path"])
        try:
            if file_path.exists():
                file_path.unlink()
                logger.info(f"  Deleted file: {file_path}")
            else:
                logger.info(f"  File already gone: {file_path}")
        except Exception as e:
            logger.warning(f"  Could not delete file: {e}")

        # Always try to delete database record
        if model["id"]:
            async with get_db_session() as session:
                result = await session.execute(
                    select(Model).where(Model.id == model["id"])
                )
                db_model = result.scalar_one_or_none()
                if db_model:
                    await session.delete(db_model)
                    await session.commit()
                    logger.info(f"  Deleted database record")

        logger.info(f"[success]Model deleted: {name}[/success]")
        return True

    async def update_last_used(self, name: str) -> None:
        """Update the last_used_at timestamp for a model."""
        async with get_db_session() as session:
            result = await session.execute(
                select(Model).where(Model.name == name)
            )
            model = result.scalar_one_or_none()
            if model:
                model.last_used_at = datetime.now()
                await session.commit()

    async def register_local_model(
        self,
        file_path: Path,
        name: Optional[str] = None,
    ) -> Model:
        """
        Register a local GGUF file that was manually added.

        Args:
            file_path: Path to the GGUF file
            name: Custom name (default: filename without extension)

        Returns:
            Created Model record
        """
        if not file_path.exists():
            raise FileNotFoundError(f"Model file not found: {file_path}")

        if not file_path.suffix == ".gguf":
            raise ValueError("Model file must be a .gguf file")

        model_name = name or file_path.stem

        return await self._register_model(
            repo_id="local",
            filename=file_path.name,
            file_path=file_path,
        )
