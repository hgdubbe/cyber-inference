# syntax=docker/dockerfile:1
# Cyber-Inference Dockerfile - NVIDIA GPU Support
#
# Build:
#   docker build -f Dockerfile.nvidia -t cyber-inference:nvidia .
#
# With custom base image:
#   docker build -f Dockerfile.nvidia --build-arg BASE_IMAGE=nvcr.io/nvidia/l4t-cuda:12.6.68-devel -t cyber-inference:jetson .

ARG BASE_IMAGE=nvcr.io/nvidia/cuda:13.0.0-devel-ubuntu24.04
FROM ${BASE_IMAGE}

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    VENV_PATH=/opt/cyber-inference/.venv \
    CYBER_INFERENCE_DATA_DIR=/app/data \
    CYBER_INFERENCE_MODELS_DIR=/app/models \
    CYBER_INFERENCE_BIN_DIR=/app/bin \
    CYBER_INFERENCE_HOST=0.0.0.0 \
    CYBER_INFERENCE_PORT=8337 \
    CYBER_INFERENCE_LOG_LEVEL=INFO \
    CYBER_INFERENCE_LLAMA_GPU_LAYERS=-1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

WORKDIR /app

COPY pyproject.toml README.md ./
COPY src/ ./src/

RUN set -eux; \
    apt-get update; \
    apt-get install -y --no-install-recommends \
        build-essential \
        cmake \
        curl \
        git \
        libgomp1 \
        python3-dev \
        python3-venv \
        python3-pip \
        wget; \
    rm -rf /var/lib/apt/lists/*; \
    python3 -m venv "${VENV_PATH}"; \
    PIP_BIN="${VENV_PATH}/bin/pip"; \
    "${PIP_BIN}" install --upgrade pip; \
    "${PIP_BIN}" install --no-cache-dir .; \
    useradd -m -o -u 1000 -s /bin/bash cyber || true; \
    mkdir -p /app/data /app/models /app/bin /app/data/logs; \
    chown -R cyber:cyber /app /opt/cyber-inference

ENV PATH="${VENV_PATH}/bin:${PATH}"

EXPOSE 8337

USER cyber

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8337/health || exit 1

VOLUME ["/app/data", "/app/models", "/app/bin"]

CMD ["python3", "-m", "cyber_inference.cli", "serve", "--host", "0.0.0.0", "--port", "8337"]
