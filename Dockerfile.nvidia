# syntax=docker/dockerfile:1
# Cyber-Inference Dockerfile - NVIDIA GPU Support
#
# Build:
#   docker build -f Dockerfile.nvidia -t cyber-inference:nvidia .
#
# With custom base image:
#   docker build -f Dockerfile.nvidia --build-arg BASE_IMAGE=nvcr.io/nvidia/l4t-cuda:12.6.68-devel -t cyber-inference:jetson .

ARG BASE_IMAGE=nvcr.io/nvidia/cuda:13.0.0-devel-ubuntu24.04
FROM ${BASE_IMAGE}

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    VENV_PATH=/opt/cyber-inference/.venv \
    CYBER_INFERENCE_DATA_DIR=/app/data \
    CYBER_INFERENCE_MODELS_DIR=/app/models \
    CYBER_INFERENCE_BIN_DIR=/app/bin \
    CYBER_INFERENCE_HOST=0.0.0.0 \
    CYBER_INFERENCE_PORT=8337 \
    CYBER_INFERENCE_LOG_LEVEL=INFO \
    CYBER_INFERENCE_LLAMA_GPU_LAYERS=-1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

WORKDIR /app

COPY pyproject.toml README.md ./
COPY src/ ./src/

RUN set -eux; \
    apt-get update; \
    apt-get install -y --no-install-recommends \
        build-essential \
        cmake \
        curl \
        git \
        libcurl4-openssl-dev \
        libgomp1 \
        python3-dev \
        python3-venv \
        python3-pip \
        wget; \
    rm -rf /var/lib/apt/lists/*; \
    # Build llama.cpp with CUDA support
    git clone --depth 1 https://github.com/ggerganov/llama.cpp /tmp/llama.cpp; \
    cmake -S /tmp/llama.cpp -B /tmp/llama.cpp/build \
        -DGGML_CUDA=ON \
        -DLLAMA_CURL=ON \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_CUDA_ARCHITECTURES="72;87;89;90"; \
    cmake --build /tmp/llama.cpp/build -j$(nproc) --target llama-server; \
    mkdir -p /app/bin; \
    # Copy llama-server and all shared libraries
    find /tmp/llama.cpp/build -name "llama-server" -type f -executable -exec cp {} /app/bin/ \; ; \
    find /tmp/llama.cpp/build -name "*.so*" -exec cp {} /app/bin/ \; ; \
    ls -la /app/bin/; \
    test -f /app/bin/llama-server || (echo "ERROR: llama-server not found!" && exit 1); \
    rm -rf /tmp/llama.cpp; \
    # Install Python app
    python3 -m venv "${VENV_PATH}"; \
    PIP_BIN="${VENV_PATH}/bin/pip"; \
    "${PIP_BIN}" install --upgrade pip; \
    "${PIP_BIN}" install --no-cache-dir .; \
    useradd -m -o -u 1000 -s /bin/bash cyber || true; \
    mkdir -p /app/data /app/models /app/data/logs; \
    chown -R cyber:cyber /app /opt/cyber-inference

ENV PATH="${VENV_PATH}/bin:${PATH}" \
    LD_LIBRARY_PATH="/app/bin:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"

EXPOSE 8337

USER cyber

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8337/health || exit 1

VOLUME ["/app/data", "/app/models", "/app/bin"]

CMD ["python3", "-m", "cyber_inference.cli", "serve", "--host", "0.0.0.0", "--port", "8337"]
